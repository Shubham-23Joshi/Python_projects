{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bf64e03-6df1-4e16-8d11-bd46cea35be8",
   "metadata": {},
   "source": [
    "# Running HR Model Single Inference on Edge TPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1063bc98-5319-4d50-9010-aa2d67198f4e",
   "metadata": {},
   "source": [
    "The following set of codes is for running a single inference of the model on Edge TPU. <br> \n",
    "One image is selected as input for the model and then the result is displayed back after execution. <br> <br>\n",
    "Note on Usage:\n",
    "This file is to be executed on an Edge TPU, typically the Coral Dev Board. A jupyter notebook has been<br> \n",
    "installed on the Coral Board under user 'Mendel'. Type 'jupyter notebook' or 'jupyter lab' in the linux<br>\n",
    "command prompt to open the code files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fe9cfa-03cd-4961-a8eb-4a12ccb7165c",
   "metadata": {},
   "source": [
    "### Importing necessary dependencies, including Pycoral API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9ada0b-aac5-401c-be4c-6da313d912ee",
   "metadata": {},
   "source": [
    "For this the pycoral API has to be installed. <link>https://coral.ai/docs/reference/py/</link> <br> \n",
    "Pycoral API and the necessary dependencies are already installed on the Coral Board (\"tuned shrimp\") <br> \n",
    "under the user 'Mendel'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd95c5c-0057-4468-ba5d-de8945589fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "from pycoral.utils import edgetpu\n",
    "from pycoral.utils import dataset\n",
    "from pycoral.adapters import common\n",
    "from pycoral.adapters import classify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aef77bc-cfd7-44c7-af96-40366dc78062",
   "metadata": {},
   "source": [
    "### Specifying the model, labels and images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22044f5e",
   "metadata": {},
   "source": [
    "Here the model specified has to be compatible with Edge TPU. Ensure the model is converted, quantized <br>\n",
    "and compiled with the Edgetpu compiler. The model file name should have 'edgetpu.tflite' in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4356e343-9769-475c-8c07-de155100493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'hr_model_quant_edgetpu.tflite' # model edgetpu-tflite file\n",
    "label_file = 'digits_labels.txt' # file with labels for classes\n",
    "image_file = '2_img.GIF' # chose your image file here for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5847686-1b63-4839-8125-aac16a6e2f78",
   "metadata": {},
   "source": [
    "### Building the interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad05fb-cdac-4722-857e-5047220769e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = edgetpu.make_interpreter(model_file)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12578acc-6c13-4aba-91bd-d60cb0280991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if the model is integer quantized or not\n",
    "if common.input_details(interpreter, 'dtype') != np.uint8:\n",
    "    raise ValueError('Only support uint8 input type.')\n",
    "else:\n",
    "    print('model is quantized, ready to use')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a6666b-dec0-4e7e-ab3b-4e84f70beb65",
   "metadata": {},
   "source": [
    "### Processing the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3020d07-07ba-43ab-8935-42391ff892a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the image size required from the interpreter\n",
    "size = common.input_size(interpreter)\n",
    "#image = Image.open(image_file).convert('L').resize(size, Image.ANTIALIAS)\n",
    "\n",
    "# resizing the image with greyscale, inverting and converting it into numpy array\n",
    "proc_img = ImageOps.invert(ImageOps.grayscale(Image.open(image_file).convert('RGB').resize(size, Image.ANTIALIAS)))\n",
    "img_array = np.array(proc_img)\n",
    "print(img_array.shape) # get the shape and verify if it fits with the model input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c4e07-d33b-4690-bda1-ed9c0700f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the images for visualization\n",
    "fig = plt.figure(figsize=(7, 3))\n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(Image.open(image_file))\n",
    "plt.axis('off')\n",
    "plt.title(\"Raw Image\")\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.axis('off')\n",
    "plt.title(\"Processed Image\")\n",
    "plt.imshow(img_array, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a44271-b7b8-4ced-b468-9b015d76c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first broadcasting the img_array to fit the dimentions (28, 28, 1) in this case\n",
    "# it depends on your model input required dimension whether you have to broadcast \n",
    "# the array or not\n",
    "# this is the recommended way, the alternative manual method is given below\n",
    "img_array = img_array.reshape((28,28,1))\n",
    "print(img_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6079d5ba-c873-4b20-aa17-657fb5a9a0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual image preprocessing - 2 transformations: normalization and quantization\n",
    "# combined in 1 line: q = (input - mean) / (std * scale) + zero_point\n",
    "# 1e-5 used instead of 0 to prevent division by 0 error\n",
    "\n",
    "# defining the parameters\n",
    "top_k = 1\n",
    "threshold = 0.0\n",
    "count = 1\n",
    "input_mean = 128.0\n",
    "input_std = 128.0\n",
    "\n",
    "params = common.input_details(interpreter, 'quantization_parameters')\n",
    "print(params)\n",
    "scale = params['scales']\n",
    "zero_point = params['zero_points']\n",
    "\n",
    "if abs(scale * std - 1) < 1e-5 and abs(mean - zero_point) < 1e-5:\n",
    "    # Input data does not require preprocessing.\n",
    "    print('no preprocessing required')\n",
    "    common.set_input(interpreter, image)\n",
    "else:\n",
    "    # Input data requires preprocessing\n",
    "    print('image preprocessed')\n",
    "    img_array = img_array.reshape((28,28,1))\n",
    "    normalized_input = (img_array - mean) / (std * scale) + zero_point\n",
    "    np.clip(normalized_input, 0, 255, out=normalized_input)\n",
    "    common.set_input(interpreter, normalized_input.astype(np.uint8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c59b8-4567-4495-8efb-34c38f7302db",
   "metadata": {},
   "source": [
    "### Running the Inference and Printing the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef95e1-df9b-4717-bfd4-b616fe781e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the inference\n",
    "common.set_input(interpreter, img_array)\n",
    "interpreter.invoke()\n",
    "classes = classify.get_classes(interpreter, top_k=1)\n",
    "print(classes) # this prints the resultant class id chosen by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a13716-937e-44c7-bb92-c67e9105a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the result\n",
    "# use this if you have a labels file\n",
    "labels = dataset.read_label_file(label_file)\n",
    "print(\"class | score\")\n",
    "for c in classes:\n",
    "    print('%s     | %.5f' % (labels.get(c.id, c.id), c.score))\n",
    "    #print(c.score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
