{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4db19478-640e-4558-aeee-4fa2c51a1f48",
   "metadata": {},
   "source": [
    "# Running HR Model Live Inference on Edge TPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1be8f7-4a5b-4ff7-bd63-85f5095aea3c",
   "metadata": {},
   "source": [
    "The following set of codes is for executing the inferencing in loop based on video feed \n",
    "from the camera. <br>Camera snapshots are taken after every specific time interval and \n",
    "the raw image is processed and fed as  <br> input to the model for inferenceing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00539313-72b1-4627-8c0f-bd6a377a5f54",
   "metadata": {},
   "source": [
    "Note on Usage:\n",
    "This file is to be executed on an Edge TPU, typically the Coral Dev Board. A jupyter notebook has been<br> \n",
    "installed on the Coral Board under user 'Mendel'. Type 'jupyter notebook' or 'jupyter lab' in the linux<br>\n",
    "command prompt to open the code files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d61af-24e8-46ed-8cd1-9d19bbed26e2",
   "metadata": {},
   "source": [
    "### Importing necessary dependencies, including Pycoral API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fe8871-cf26-46f0-afac-e13fd3e8ee91",
   "metadata": {},
   "source": [
    "For this the pycoral API has to be installed. <link>https://coral.ai/docs/reference/py/</link> <br> \n",
    "Pycoral API and the necessary dependencies are already installed on the Coral Board (\"tuned shrimp\") <br> \n",
    "under the user 'Mendel'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bbf32e-3625-481a-b9d6-6f9b6497b2fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "VideoStream library is used from 'imutils' to display the live camera feed. Use 'pip install imutils' <br> \n",
    "for using the VideoStream. Alternatively, OpenCV.VideoCapture() can also be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af6df2d-10ba-4dcf-99fc-ab21119febe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "from IPython.display import clear_output\n",
    "#import ipywidgets as widget\n",
    "import threading\n",
    "import numpy as np\n",
    "from pycoral.utils import edgetpu\n",
    "from pycoral.utils import dataset\n",
    "from pycoral.adapters import common\n",
    "from pycoral.adapters import classify\n",
    "from PIL import Image, ImageOps\n",
    "import imutils\n",
    "from imutils.video import VideoStream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddc202f-1a21-41f0-aa01-b7f99b1a236a",
   "metadata": {},
   "source": [
    "### Specifying the model, labels and images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98927291-875b-49c6-bdb2-b1861a1620dd",
   "metadata": {},
   "source": [
    "Here the model specified has to be compatible with Edge TPU. Ensure the model is converted, quantized <br>\n",
    "and compiled with the Edgetpu compiler. The model file name should have 'edgetpu.tflite' in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955384f4-0360-4101-a523-1b420eb29e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'hr_model_quant_edgetpu.tflite' # model edgetpu-tflite file\n",
    "label_file = 'digits_labels.txt' # file with labels for classes\n",
    "image_file = '2_img.GIF' # chose your image file here for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1e51ba-f811-4f26-a6aa-3549ed7852fe",
   "metadata": {},
   "source": [
    "### Building the interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95a22a1-39a3-47a0-b628-6d91609a8c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TF interpreter\n",
    "interpreter = edgetpu.make_interpreter(model_file)\n",
    "interpreter.allocate_tensors()\n",
    "size = common.input_size(interpreter)\n",
    "#run the below code to extract labels\n",
    "# labels = dataset.read_label_file(label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a88f06-a93b-4823-806b-e1ebd4338996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if the model is integer quantized or not\n",
    "if common.input_details(interpreter, 'dtype') != np.uint8:\n",
    "    raise ValueError('Only support uint8 input type.')\n",
    "else:\n",
    "    print('model is quantized, ready to use')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2849a17-ec1d-4a7a-a2c3-45acccd3e7cd",
   "metadata": {},
   "source": [
    "### Initiating the VideoStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b873899-9e0f-4dfd-875d-d95f2c2a1144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise video streamer\n",
    "vs = VideoStream(src=1).start() # change the source according to which camera you are using\n",
    "print(\"starting video stream...\")\n",
    "time.sleep(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585427e8-cc15-4ed5-9718-53f0af659c5f",
   "metadata": {},
   "source": [
    "## Running the Inference in Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd8b8ff-1e87-4e3a-bd56-bf03d1eb07e7",
   "metadata": {},
   "source": [
    "Note: the inference model yields good accuracy only if the captured images have minimal noises and <br> high contrast. For this the handwriting figures should be in dark color and on a plane white paper or background. <br> There should be enough brigh light and no shadows cast on the figures if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb23cede-28b7-45cd-a256-d0a52292550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # take the frame from the threaded video stream and resize to maximum width of 500 pixels\n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame, width=500)\n",
    "    \n",
    "    # prepare the frame for classification by converting it from\n",
    "    # BGR to Gray channel ordering and then from a NumPy array to PIL image format\n",
    "    frame = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # convert the image to binary with threshold value of 120 for easier inference\n",
    "    th, im_th = cv2.threshold(frame, 120, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    img = (Image.fromarray(im_th))\n",
    "    # convert the image to numpy array\n",
    "    \n",
    "    frame_arr = np.array(img.resize(size, Image.ANTIALIAS))\n",
    "    # broadcasting frame_img to fit dimentions (28, 28, 1)\n",
    "    frame_arr_trfm = frame_arr.reshape((28,28,1))\n",
    "    \n",
    "    # execute the inference\n",
    "    common.set_input(interpreter, frame_arr_trfm)\n",
    "    interpreter.invoke()\n",
    "    classes = classify.get_classes(interpreter, top_k=1)\n",
    "    # display stream and print result\n",
    "    plt.imshow(frame) # original captured frame\n",
    "    plt.show()\n",
    "    plt.imshow(im_th, cmap='gray') # binary image\n",
    "    plt.show()\n",
    "    print(classes)\n",
    "    plt.imshow(frame_arr) # final resized image\n",
    "    plt.show() \n",
    "\n",
    "    time.sleep(1) # delay\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "cv.destroyAllWindows()\n",
    "vs.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
